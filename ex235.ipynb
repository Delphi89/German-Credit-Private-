{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n",
      "Torchvision Version:  0.4.0a0+6b959ee\n",
      "Using 2 NVIDIA 1080TI GPUs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "from random import randint\n",
    "from Utillities import Utillities\n",
    "from cnn_model import CNN6     \n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "if os.path.exists(\"checkpoint.pt\"):\n",
    "    os.remove(\"checkpoint.pt\")\n",
    "\n",
    "torch.manual_seed(10)   # reproducible\n",
    "\n",
    "OPTIMIZATION_PLUGIN = 'Bayesian' # 'Bayesian' or 'Scikit' or 'GradDescent'\n",
    "#Bayesian requires: $ conda install -c conda-forge bayesian-optimization\n",
    "\n",
    "GET_STATS = False\n",
    "GPU_SELECT = 2 # can be 0, 1, 2 (both)\n",
    "PARALLEL_PROCESSES = 2\n",
    "TRIALS = 30\n",
    "RANDOM_STARTS = 300\n",
    "LR  = 1e-5                    # learning rate\n",
    "SCI_LR =  1e-5\n",
    "LR2 = 1e-5\n",
    "SCI_MM = 0.5                  # momentum - used only with SGD optimizer\n",
    "MM = 0.5\n",
    "L_FIRST = 14                  # initial number of channels\n",
    "KERNEL_X = 14\n",
    "patience = 7                 # if validation loss not going down, wait \"patience\" number of epochs\n",
    "accuracy = 0\n",
    "MaxPercent = 0\n",
    "\n",
    "PercentVector = np.zeros(RANDOM_STARTS + TRIALS)\n",
    "PercentVec = np.zeros(RANDOM_STARTS + TRIALS)\n",
    "count = 0\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.enable()\n",
    "    \n",
    "\n",
    "if GPU_SELECT == 2:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using\", torch.cuda.device_count(), \"NVIDIA 1080TI GPUs!\")\n",
    "\n",
    "if GPU_SELECT == 1:\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    print(\"Using one (the second) NVIDIA 1080TI GPU!\")\n",
    "\n",
    "if GPU_SELECT == 0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")       \n",
    "    print(\"Using one (the first) NVIDIA 1080TI GPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from early_stopping import EarlyStopping\n",
    "from dataset import dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)  # initialize the early_stopping object\n",
    "\n",
    "# Counter for the execution time\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if OPTIMIZATION_PLUGIN == 'Scikit' :\n",
    "    from skopt import gp_minimize\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from skopt.space import Real, Integer\n",
    "    from skopt.utils import use_named_args\n",
    "    from skopt.plots import plot_convergence\n",
    "    from functools import partial\n",
    "    from skopt.plots import plot_evaluations\n",
    "    from skopt import gp_minimize, forest_minimize, dummy_minimize, gbrt_minimize\n",
    "    from skopt.plots import plot_objective\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from skopt.space import Real, Categorical, Integer\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "    SCI_LR = Categorical(categories=[1e-1, 3e-1, 5e-1, 7e-1, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.08, 0.09],name= 'SCI_LR')\n",
    "    SCI_MM = Categorical(categories=[0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999], name='SCI_MM')\n",
    "    SCI_REGULARIZATION = Categorical(categories=[0.0001, 0.0003, 0.0007, 0.001, 0.003, 0.007, 0.01, 0.03, 0.07, 0.1, 0.3, 0.7], name='SCI_REGULARIZATION')\n",
    "    SCI_EPOCHS = Categorical(categories=[2000, 1000], name='SCI_EPOCHS')\n",
    "    SCI_optimizer = Categorical(categories=['Adam', 'AMSGrad', 'SGD', 'RMSprop', 'Rprop', 'AdamW', 'ASGD', 'Adadelta', 'Adamax'],name='SCI_optimizer') #\n",
    "    SCI_loss_type = Categorical(categories=['CrossEntropyLoss', 'MultiMarginLoss','NLLLoss', 'L1Loss'],name='SCI_loss_type') # \n",
    "    SCI_BATCH_SIZE = Categorical(categories=[4, 8, 12, 16, 24, 32, 48, 64, 80, 96, 104, 128, 144, 160, 192, 224, 256], name='SCI_BATCH_SIZE')\n",
    "    SCI_DROPOUT = Categorical(categories=[0, 0.01, 0.03, 0.07, 0.1, 0.13, 0.17, 0.2, 0.23, 0.27, 0.3, 0.33, 0.37, 0.4] , name = 'SCI_DROPOUT')\n",
    "    SCI_RELU = Categorical(categories=['True', 'False'] , name = 'SCI_RELU')\n",
    "    SCI_BIAS = Categorical(categories=['True', 'False'] , name = 'SCI_BIAS')\n",
    "    SCI_L_SECOND = Categorical(categories=[2, 4, 6, 8, 12, 16, 20, 24, 32, 48, 64], name='SCI_L_SECOND')\n",
    "    SCI_BN_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_BN_MOMENTUM') \n",
    "    SCI_SGD_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_SGD_MOMENTUM') \n",
    "    SCI_LINEARITY = Categorical(categories=[1, 2],name= 'SCI_LINEARITY')\n",
    "   \n",
    "\n",
    "    dimensions = [SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM, SCI_LINEARITY]\n",
    "\n",
    "    @use_named_args(dimensions = dimensions)\n",
    "\n",
    "    def objective(SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM, SCI_LINEARITY):\n",
    "        global device  \n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 'CrossEntropyLoss':\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "            if LOSS == 'NLLLoss':\n",
    "                loss_func = nn.NLLLoss()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        optimizer = str(SCI_optimizer)\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    " \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "                \n",
    "        cnn = CNN6(L_FIRST, SCI_L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, SCI_DROPOUT, dataset.CLASSES, SCI_LINEARITY)     \n",
    "\n",
    "        optimizer = Utillities.optimization_algorithms(SCI_optimizer,cnn, SCI_LR, SCI_SGD_MOMENTUM,\n",
    "                                                       SCI_REGULARIZATION)\n",
    "        \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim=0) \n",
    "            cnn = cnn.cuda()\n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.apply(CNN6.weights_reset)\n",
    "        cnn.share_memory()\n",
    "     \n",
    "\n",
    "        Utillities.listing(optimizer, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, SCI_L_SECOND, SCI_LR, SCI_RELU, SCI_BIAS, SCI_loss_type, REGULARIZATION, SCI_BATCH_SIZE, SCI_DROPOUT, SCI_LINEARITY)\n",
    "    \n",
    "        #SCI_BATCH_SIZE = 1\n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, pin_memory=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    #ps = torch.exp(output)\n",
    "                    #equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    #accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "       \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    #nn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])\n",
    "            print('Class: ',i,' correct: ', class_correct[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1] * 5)\n",
    "    \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 300\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "        print('Credit Cost: ',CreditCost)\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        \n",
    "        return CreditCost\n",
    "    \n",
    "    #not working #res_gp = gp_minimize(objective, dimensions=dimensions, n_calls=TRIALS, random_state=1, verbose=True, acq_func='gp_hedge', acq_optimizer='auto', n_jobs=1)\n",
    "    res_gp = forest_minimize(objective, dimensions=dimensions, base_estimator='RF', n_calls=TRIALS, n_random_starts=RANDOM_STARTS, acq_func='EI', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=10000, xi=0.01, kappa=1.5, n_jobs=4)\n",
    "    #res_gp = gbrt_minimize(objective, dimensions=dimensions, base_estimator='ET', n_calls=TRIALS+RANDOM_STARTS, n_random_starts=RANDOM_STARTS, acq_func='LCB', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=100, xi=0.01, kappa=1.96, n_jobs=1)\n",
    "    #res_gp = dummy_minimize(objective, dimensions=dimensions, n_calls=TRIALS, x0=None, y0=None, random_state=None, verbose=True, callback=None)      \n",
    "\n",
    "    \"Best score=%.4f\" % res_gp.fun\n",
    "    print(\"\"\"Best parameters: - optimization=%d\"\"\" % (res_gp.x[0]))\n",
    "  \n",
    "    print(res_gp)\n",
    "    plot_convergence(res_gp)\n",
    "    #plot_evaluations(res_gp)\n",
    "    #plot_objective(res_gp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | SCI_BA... | SCI_BIAS  | SCI_BN... | SCI_DR... | SCI_EP... | SCI_LI... |  SCI_LR   | SCI_L_... |  SCI_MM   | SCI_RE... | SCI_RELU  | SCI_SG... | SCI_lo... | SCI_op... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "*** LOSS ******: 1\n",
      "*********  CrossEntropyLoss\n",
      "Optimization:  Adamax (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.11184\n",
      "    weight_decay: 0.3772178321289495\n",
      ")\n",
      "Batch Normalization Momentum:  0.0\n",
      "Nodes:  45\n",
      "LR:  0.11184\n",
      "RELU:  True\n",
      "BIAS:  False\n",
      "Loss Type:  1\n",
      "REGULARIZATION:  0.3772178321289495\n",
      "BATCH_SIZE:  54\n",
      "Dropout:  0.21\n",
      "Final Linear Layers:  1\n",
      "average loss: 56.970884\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0  of  tensor(114.)\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0  of  tensor(86.)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dx' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (54.544772592524325, 2.4334457419498947, 0.00011323106917143777, 0.21163280084228783, 1146.7558908171131, 1.2760923983587054, 0.11183750080546477, 45.54065160742402, 0.3969739392822086, 0.3772178321289495, 1.8341970836625567, 0.6783673053927919, 1.6113122266972373, 8.894275753154599)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e94a728e3e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m#acq=\"ucb\", kappa=0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ei\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     )\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e94a728e3e07>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(SCI_RELU, SCI_BIAS, SCI_loss_type, SCI_optimizer, SCI_LR, SCI_MM, SCI_REGULARIZATION, SCI_EPOCHS, SCI_BATCH_SIZE, SCI_DROPOUT, SCI_L_SECOND, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM, SCI_LINEARITY)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Class: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' correct: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' of '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTESTED_ELEMENTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Correct Results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dx' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if OPTIMIZATION_PLUGIN == 'Bayesian' :\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    \n",
    "    #def black_box_function(x, y):\n",
    "    def objective(SCI_RELU, SCI_BIAS, SCI_loss_type,\n",
    "                  SCI_optimizer, SCI_LR, SCI_MM, \n",
    "                  SCI_REGULARIZATION, SCI_EPOCHS, SCI_BATCH_SIZE, \n",
    "                  SCI_DROPOUT, SCI_L_SECOND, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM, SCI_LINEARITY):\n",
    "        global count, PercentVector, PercentVec, device, MaxPercent\n",
    "\n",
    "        \n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)                    # integer between 4 and 256\n",
    "        SCI_MM = round(SCI_MM,3)                                # real with three decimals between (0.001, 0.999)\n",
    "        SCI_LR = round(SCI_LR,5)                                # real with five decimals between(1e-4, 7e-1)            \n",
    "        SCI_DROPOUT = round(SCI_DROPOUT,2)                      # real with two decimals between (0, 0.4)\n",
    "        SCI_L_SECOND = int(SCI_L_SECOND)                        # integer between 2 and 64\n",
    "        SCI_EPOCHS = int(SCI_EPOCHS)                            # integer between (100, 500)\n",
    "        SCI_BN_MOMENTUM = round(SCI_BN_MOMENTUM,2)              # real with two decimals between (0, 0.99)\n",
    "        SCI_SGD_MOMENTUM = round(SCI_SGD_MOMENTUM,2)            # real with two decimals between (0, 0.99) \n",
    "        SCI_loss_type = int(SCI_loss_type)                      # integer between 1 and 3 ('CrossEntropyLoss', 'MultiMarginLoss','NLLLoss')\n",
    "        SCI_LINEARITY = int(SCI_LINEARITY)\n",
    "        if int(SCI_RELU) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_RELU = True      \n",
    "        else:\n",
    "            SCI_RELU = False      \n",
    "        if int(SCI_BIAS) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_BIAS = True      \n",
    "        else:\n",
    "            SCI_BIAS = False  \n",
    " \n",
    "        SCI_REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        \n",
    "        cnn = CNN6(L_FIRST, SCI_L_SECOND, KERNEL_X,\n",
    "                   SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU,\n",
    "                   SCI_DROPOUT, dataset.CLASSES, SCI_LINEARITY)     \n",
    "\n",
    "        optimizer = Utillities.optimization_algorithms(SCI_optimizer,cnn, SCI_LR, SCI_SGD_MOMENTUM,\n",
    "                                                       SCI_REGULARIZATION)\n",
    "        \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim = 0) \n",
    "            cnn = cnn.cuda()                \n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.apply(CNN6.weights_reset)        \n",
    "        cnn.share_memory()\n",
    "     \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            print('*** LOSS ******:',  LOSS)\n",
    "            if LOSS == 1:\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                print('*********  CrossEntropyLoss')\n",
    "            if LOSS == 2:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "                print('*********  MMLoss')                               \n",
    "            if LOSS == 3:\n",
    "                loss_func = nn.NLLLoss() \n",
    "                print('*********  NLLLoss')                 \n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "    \n",
    "        Utillities.listing(optimizer, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, \n",
    "                           SCI_L_SECOND, SCI_LR, SCI_RELU, \n",
    "                           SCI_BIAS, SCI_loss_type, SCI_REGULARIZATION, \n",
    "                           SCI_BATCH_SIZE, SCI_DROPOUT, SCI_LINEARITY)\n",
    "\n",
    "    \n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = 90, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = 300, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                running_loss += valid_loss\n",
    "                if epoch % 100 == 0: \n",
    "                    print('average loss: %.6f' %(running_loss))\n",
    "                    #print('classification accuracy: %.6f' %())\n",
    "                    running_loss = 0.0\n",
    "                   \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "        \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    #cnn = TheModelClass(*args, **kwargs)\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt'))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                dx = ((c.cpu()).numpy()).astype(int)\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])   \n",
    "            print('Class: ',i,' correct: ', class_correct[i],' of ',dataset.TESTED_ELEMENTS[i])\n",
    "\n",
    "        plt.matshow(dx.reshape((10, 20)))\n",
    "        plt.ylabel('Correct Results')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1])\n",
    "        \n",
    "        #if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "        #    percent = percent + 30\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        if -percent < MaxPercent : \n",
    "            MaxPercent = -percent\n",
    "        print('Best Score So Far: ',MaxPercent)    \n",
    "        \n",
    "        PercentVector[count] = MaxPercent    \n",
    "        PercentVec[count] = count\n",
    "        # plot the data\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.plot(PercentVec, -PercentVector, color='tab:blue')\n",
    "        #print(CreditVec, -CreditVector)\n",
    "        count = count + 1\n",
    "        # display the plot\n",
    "        plt.show()\n",
    "        \n",
    "        return -percent\n",
    "    \n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        #pbounds=pbounds,\n",
    "        pbounds={'SCI_RELU': (1,2.99), \n",
    "                 'SCI_BIAS': (1,2.99), \n",
    "                 'SCI_loss_type': (1, 3.99), \n",
    "                 'SCI_optimizer': (1, 9.99),\n",
    "                 'SCI_LR': (0.0001, 0.6), \n",
    "                 'SCI_MM': (0.001, 0.999), \n",
    "                 'SCI_REGULARIZATION': (0.0001, 0.7), \n",
    "                 'SCI_EPOCHS': (1000, 2000), \n",
    "                 'SCI_BATCH_SIZE': (2, 128), \n",
    "                 'SCI_DROPOUT': (0, 0.7), \n",
    "                 'SCI_L_SECOND': (2, 128), \n",
    "                 'SCI_BN_MOMENTUM': (0, 0.99), \n",
    "                 'SCI_SGD_MOMENTUM': (0, 0.99), \n",
    "                 'SCI_LINEARITY': (1,3.99)},\n",
    "        verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "        random_state=1,\n",
    "    )\n",
    "        \n",
    "\n",
    "    #optimizer.maximize(\n",
    "        #n_iter=TRIALS, acq=\"ucb\", kappa=0.1\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points = RANDOM_STARTS,\n",
    "        n_iter = TRIALS,\n",
    "        #acq=\"ucb\", kappa=0.1\n",
    "        \n",
    "        acq=\"ei\", xi=1e-4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(optimizer.max)\n",
    "    \n",
    "    for i, res in enumerate(optimizer.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZATION_PLUGIN == 'GradDescent' :\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import torch.optim as optim\n",
    "    from torch.autograd import Variable\n",
    "    from Utillities import Utillities\n",
    "    from cnn_model import CNN6      \n",
    "\n",
    "    #SCI_LR = 0.2\n",
    "    SCI_REGULARIZATION = 0.03\n",
    "    SCI_EPOCHS = 200\n",
    "    SCI_RELU = 'True'\n",
    "    SCI_BIAS = 'True'\n",
    "    SCI_BN_MOMENTUM = 0.1\n",
    "    \n",
    "    \n",
    "    SCI_loss_type = torch.randint(3, 4, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_loss_type: ',SCI_loss_type)    \n",
    "    SCI_SGD_MOMENTUM = torch.rand(1, requires_grad=True)\n",
    "    print('SCI_SGD_MOMENTUM: ', SCI_SGD_MOMENTUM)\n",
    "    SCI_BATCH_SIZE   = torch.randint(128, 256, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_BATCH_SIZE: ',SCI_BATCH_SIZE)\n",
    "    SCI_L_SECOND   = torch.randint(80, 96, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_L_SECOND: ',SCI_L_SECOND)\n",
    "    SCI_optimizer   = torch.randint(6, 11, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_optimizer: ',SCI_optimizer)    \n",
    "    SCI_DROPOUT      = torch.rand(1, requires_grad=True)    \n",
    "    print('SCI_DROPOUT: ',SCI_DROPOUT)   \n",
    "    SCI_LR      = torch.rand(1, requires_grad=True)    \n",
    "    print('SCI_LR: ',SCI_LR) \n",
    "    \n",
    "\n",
    "    def objective(SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND, SCI_optimizer, LINEARITY, SCI_loss_type, SCI_LR):\n",
    "        global SCI_REGULARIZATION, SCI_EPOCHS, SCI_RELU\n",
    "        global SCI_BIAS, SCI_BN_MOMENTUM, device, MaxCredit, count, CreditVector, CreditVec\n",
    "        \n",
    "        SCI_SGD_MOMENTUM = SCI_SGD_MOMENTUM/10\n",
    "        DROPOUT = (SCI_DROPOUT/2).item()\n",
    "        if SCI_DROPOUT < 0 :\n",
    "            DROPOUT = 0\n",
    "            \n",
    "        if SCI_BATCH_SIZE < 2 :\n",
    "            SCI_BATCH_SIZE = 64            \n",
    "\n",
    "        BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        \n",
    "        if SCI_L_SECOND < 4 :\n",
    "            SCI_L_SECOND = 64\n",
    "            \n",
    "        \n",
    "        L_SECOND = int(SCI_L_SECOND)\n",
    "        \n",
    "        def create_loss(LOSS):   \n",
    "            print('*** LOSS ******:',  LOSS)\n",
    "            if LOSS == 1:\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "                print('*********  CrossEntropyLoss')\n",
    "            if LOSS == 2:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "                print('*********  MMLoss')                               \n",
    "            if LOSS == 3:\n",
    "                loss_func = nn.NLLLoss() \n",
    "                print('*********  NLLLoss')                 \n",
    "            return loss_func\n",
    "\n",
    "        SCI_LR = SCI_LR.item() / 3\n",
    "        print('SCI_LR: ', SCI_LR)\n",
    "        print('SCI_LR type: ', type(SCI_LR))\n",
    "\n",
    "\n",
    "        \n",
    "        loss_type = int(SCI_loss_type)\n",
    "        loss_func = create_loss(loss_type)\n",
    "        print('LOSS FUNCTION IS: ',loss_func)\n",
    "\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "\n",
    "        cnn = CNN6(L_FIRST, L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, DROPOUT, dataset.CLASSES, LINEARITY)     \n",
    "\n",
    "        optimizer1 = Utillities.optimization_algorithms(SCI_optimizer.detach().numpy(),cnn, SCI_LR, SCI_SGD_MOMENTUM, SCI_REGULARIZATION)\n",
    "        \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim=0) \n",
    "            cnn = cnn.cuda()\n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.apply(CNN6.weights_reset)\n",
    "        cnn.share_memory()\n",
    "\n",
    "\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "\n",
    "    \n",
    "        Utillities.listing(optimizer1, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, L_SECOND, SCI_LR, SCI_RELU, SCI_BIAS, loss_type, REGULARIZATION, BATCH_SIZE, DROPOUT, LINEARITY)\n",
    "\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = 144, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = 599, shuffle = False, num_workers = 0, pin_memory=True, drop_last=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                   \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())            \n",
    "                loss.backward()                             \n",
    "                optimizer1.zero_grad()\n",
    "                optimizer1.step()                           \n",
    "      \n",
    "            cnn.eval().cuda()                   \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()  \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "       \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])\n",
    "            print('Class: ',i,' correct: ', class_correct[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = (1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1] * 5\n",
    "    \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 300\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "   \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        \n",
    "        CreditCost = CreditCost + (SCI_SGD_MOMENTUM + SCI_DROPOUT + SCI_BATCH_SIZE + SCI_L_SECOND + SCI_optimizer + SCI_loss_type+ SCI_LR)/1000\n",
    "        print('Credit Cost: ',CreditCost)\n",
    "        \n",
    "        \n",
    "        if -CreditCost > MaxCredit : \n",
    "            MaxCredit = -CreditCost\n",
    "        print('Best Score So Far: ',MaxCredit)   \n",
    "        \n",
    "        CreditVector[count] = MaxCredit    \n",
    "        CreditVec[count] = count\n",
    "        # plot the data\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.plot(CreditVec, -CreditVector, color='tab:orange')\n",
    "        #print(CreditVec, -CreditVector)\n",
    "        count = count + 1\n",
    "        # display the plot\n",
    "        plt.show()\n",
    "             \n",
    "        return CreditCost\n",
    "\n",
    "    \n",
    "    def loss(y_predicted, expected):\n",
    "        return (y_predicted - expected).sum()\n",
    "            \n",
    "    \n",
    "    expected = 250\n",
    "    \n",
    "    optim_alg = optim.Adadelta([       \n",
    "        {'params': SCI_SGD_MOMENTUM, 'lr': 0.9},\n",
    "        {'params': SCI_DROPOUT, 'lr': 1e-2},\n",
    "        {'params': SCI_BATCH_SIZE, 'lr': 1},\n",
    "        {'params': SCI_L_SECOND, 'lr': 1},\n",
    "        {'params': SCI_optimizer, 'lr': 0.9},\n",
    "        {'params': SCI_loss_type, 'lr': 0.6}, \n",
    "        {'params': SCI_LR, 'lr': 0.1}   \n",
    "        ]) \n",
    "    \n",
    "    LINEARITY = 2\n",
    "    # Main optimization loop\n",
    "    for t in range(RANDOM_STARTS + TRIALS):\n",
    "        optim_alg.zero_grad()\n",
    "        y_predicted = objective(SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND, SCI_optimizer, LINEARITY, SCI_loss_type, SCI_LR)\n",
    "        current_loss = loss(y_predicted, expected)\n",
    "        current_loss.backward()\n",
    "        optim_alg.step()\n",
    "        print(f\"t = {t}, loss = {current_loss}, SCI_DROPOUT = {SCI_DROPOUT.detach().numpy()}, SCI_SGD_MOMENTUM = {SCI_SGD_MOMENTUM.item()}, SCI_BATCH_SIZE = {SCI_BATCH_SIZE.detach().numpy()}, SCI_L_SECOND = {SCI_L_SECOND.detach().numpy()}, SCI_optimizer = {SCI_optimizer.detach().numpy()}, SCI_loss_type = {SCI_loss_type.detach().numpy()}, SCI_LR = {SCI_LR.detach().numpy()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end.record()\n",
    "\n",
    "#print('Minimum Credit Cost: ',Min_Credit_Cost)\n",
    "\n",
    "print()\n",
    "print('Total execution time (minutes): ',start.elapsed_time(end)/60000)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = SortKey.CUMULATIVE\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
