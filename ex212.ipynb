{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.1.0\n",
      "Torchvision Version:  0.3.0\n",
      "Using 2 NVIDIA 1080TI GPUs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "from random import randint\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "torch.manual_seed(1)   # reproducible\n",
    "\n",
    "OPTIMIZATION_PLUGIN = 'GradDescent' # 'Bayesian' or 'Scikit' or 'GradDescent'\n",
    "#Bayesian requires: $ conda install -c conda-forge bayesian-optimization\n",
    "\n",
    "GET_STATS = False\n",
    "GPU_SELECT = 2 # can be 0, 1, 2 (both)\n",
    "PARALLEL_PROCESSES = 2\n",
    "TRIALS = 20\n",
    "RANDOM_STARTS = 20\n",
    "LR  = 1e-5                    # learning rate\n",
    "SCI_LR =  1e-5\n",
    "LR2 = 1e-5\n",
    "SCI_MM = 0.5                  # momentum - used only with SGD optimizer\n",
    "MM = 0.5\n",
    "L_FIRST = 24                  # initial number of channels\n",
    "KERNEL_X = 24\n",
    "patience = 7                 # if validation loss not going down, wait \"patience\" number of epochs\n",
    "accuracy = 0\n",
    "MaxCredit = -800\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.enable()\n",
    "    \n",
    "\n",
    "if GPU_SELECT == 2:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using\", torch.cuda.device_count(), \"NVIDIA 1080TI GPUs!\")\n",
    "\n",
    "if GPU_SELECT == 1:\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    print(\"Using one (the second) NVIDIA 1080TI GPU!\")\n",
    "\n",
    "if GPU_SELECT == 0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")       \n",
    "    print(\"Using one (the first) NVIDIA 1080TI GPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from early_stopping import EarlyStopping\n",
    "from dataset import dataset\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)  # initialize the early_stopping object\n",
    "\n",
    "# Counter for the execution time\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if OPTIMIZATION_PLUGIN == 'Scikit' :\n",
    "    from skopt import gp_minimize\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from skopt.space import Real, Integer\n",
    "    from skopt.utils import use_named_args\n",
    "    from skopt.plots import plot_convergence\n",
    "    from functools import partial\n",
    "    from skopt.plots import plot_evaluations\n",
    "    from skopt import gp_minimize, forest_minimize, dummy_minimize, gbrt_minimize\n",
    "    from skopt.plots import plot_objective\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import CategoricalEncoder\n",
    "    from skopt.space import Real, Categorical, Integer\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "    #SCI_LR = Categorical(categories=[1e-1, 3e-1, 5e-1, 7e-1, 1e-2, 3e-2, 5e-2, 7e-2, 1e-3, 3e-3, 5e-3, 7e-3, 1e-4, 3e-4, 0.1, 0.2, 0.3, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.001, 0.0001, 1e-5],name= 'SCI_LR')\n",
    "    SCI_LR = Categorical(categories=[1e-1, 3e-1, 5e-1, 7e-1, 1e-2, 3e-2, 5e-2, 7e-2, 0.1, 0.2, 0.3, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.001],name= 'SCI_LR')\n",
    "    SCI_MM = Categorical(categories=[0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999], name='SCI_MM')\n",
    "    SCI_REGULARIZATION = Categorical(categories=[0.0001, 0.0003, 0.0007, 0.001, 0.003, 0.007, 0.01, 0.03, 0.07, 0.1, 0.3, 0.7], name='SCI_REGULARIZATION')\n",
    "    SCI_EPOCHS = Categorical(categories=[2000, 1000], name='SCI_EPOCHS')\n",
    "    SCI_optimizer = Categorical(categories=['Adam', 'Adadelta', 'SGD', 'Adagrad', 'AMSGrad', 'AdamW'],name='SCI_optimizer') #\n",
    "    SCI_loss_type = Categorical(categories=['CrossEntropyLoss', 'MultiMarginLoss','NLLLoss'],name='SCI_loss_type') # \n",
    "    SCI_BATCH_SIZE = Categorical(categories=[4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 160, 192, 224, 256], name='SCI_BATCH_SIZE')\n",
    "    SCI_DROPOUT = Categorical(categories=[0, 0.01, 0.03, 0.07, 0.1, 0.13, 0.17, 0.2, 0.23, 0.27, 0.3, 0.33, 0.37, 0.4] , name = 'SCI_DROPOUT')\n",
    "    SCI_RELU = Categorical(categories=['True', 'False'] , name = 'SCI_RELU')\n",
    "    SCI_BIAS = Categorical(categories=['True', 'False'] , name = 'SCI_BIAS')\n",
    "    SCI_L_SECOND = Categorical(categories=[2, 4, 6, 8, 12, 16, 20, 24, 32, 48, 64], name='SCI_L_SECOND')\n",
    "    SCI_BN_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_BN_MOMENTUM') \n",
    "    SCI_SGD_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_SGD_MOMENTUM') \n",
    "\n",
    "    dimensions = [SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM]\n",
    "\n",
    "    @use_named_args(dimensions = dimensions)\n",
    "\n",
    "    def objective(SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM):\n",
    "        global device  \n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 'CrossEntropyLoss':\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "            if LOSS == 'NLLLoss':\n",
    "                loss_func = nn.NLLLoss()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        optimizer = str(SCI_optimizer)\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "        \n",
    "        \n",
    "        \n",
    "        from cnn_model import CNN6      \n",
    "        cnn = CNN6(L_FIRST, SCI_L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, SCI_DROPOUT, dataset.CLASSES)     \n",
    "    \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim=0) \n",
    "            cnn = cnn.cuda()\n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.apply(CNN6.weights_reset)\n",
    "        cnn.share_memory()\n",
    "     \n",
    "\n",
    "        \n",
    "        from adamw import AdamW\n",
    "        \n",
    "        \n",
    "        if SCI_optimizer == 'Adam':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'AMSGrad':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION, amsgrad=True)\n",
    "        if SCI_optimizer == 'AdamW':\n",
    "            optimizer = AdamW(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay = REGULARIZATION)            \n",
    "        if SCI_optimizer == 'SGD':\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=LR, momentum=SCI_SGD_MOMENTUM, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'Adadelta':\n",
    "            optimizer = optim.Adadelta(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'Adagrad':\n",
    "            optimizer = optim.Adagrad(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "    \n",
    "        from Utillities import Utillities\n",
    "        Utillities.listing(optimizer, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, SCI_L_SECOND, SCI_LR, SCI_RELU, SCI_BIAS, SCI_loss_type, REGULARIZATION, SCI_BATCH_SIZE, SCI_DROPOUT)\n",
    "    \n",
    "        #SCI_BATCH_SIZE = 1\n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, pin_memory=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    ps = torch.exp(output)\n",
    "                    equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "       \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    #nn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])\n",
    "            print('Class: ',i,' correct: ', class_correct[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1] * 5)\n",
    "    \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 300\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "        print('Credit Cost: ',CreditCost)\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        \n",
    "        return CreditCost\n",
    "    \n",
    "    #   not working    #res_gp = gp_minimize(objective, dimensions=dimensions, n_calls=TRIALS, random_state=1, verbose=True, acq_func='gp_hedge', acq_optimizer='auto', n_jobs=1)\n",
    "    #res_gp = forest_minimize(objective, dimensions=dimensions, base_estimator='RF', n_calls=TRIALS, n_random_starts=RANDOM_STARTS, acq_func='EI', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=10000, xi=0.01, kappa=1.96, n_jobs=128)\n",
    "    res_gp = gbrt_minimize(objective, dimensions=dimensions, base_estimator='ET', n_calls=TRIALS+RANDOM_STARTS, n_random_starts=RANDOM_STARTS, acq_func='LCB', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=100, xi=0.01, kappa=1.96, n_jobs=1)\n",
    "    #res_gp = dummy_minimize(objective, dimensions=dimensions, n_calls=TRIALS, x0=None, y0=None, random_state=None, verbose=True, callback=None)      \n",
    "\n",
    "    \"Best score=%.4f\" % res_gp.fun\n",
    "    print(\"\"\"Best parameters: - optimization=%d\"\"\" % (res_gp.x[0]))\n",
    "  \n",
    "    print(res_gp)\n",
    "    plot_convergence(res_gp)\n",
    "    #plot_evaluations(res_gp)\n",
    "    #plot_objective(res_gp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if OPTIMIZATION_PLUGIN == 'Bayesian' :\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    \n",
    "    #def black_box_function(x, y):\n",
    "    def objective(SCI_RELU, SCI_BIAS, SCI_loss_type, SCI_optimizer, SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_LR, SCI_DROPOUT, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM):\n",
    "        global device, MaxCredit  \n",
    "        \n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)                    # integer between 4 and 256\n",
    "        SCI_MM = round(SCI_MM,3)                                # real with three decimals between (0.001, 0.999)\n",
    "        SCI_REGULARIZATION = round(SCI_REGULARIZATION,3)        # real with three decimals between (0.001, 0.7)\n",
    "        SCI_LR = round(SCI_LR,5)                                # real with five decimals between(1e-4, 7e-1)            \n",
    "        SCI_DROPOUT = round(SCI_DROPOUT,2)                      # real with two decimals between (0, 0.4)\n",
    "        SCI_L_SECOND = int(SCI_L_SECOND)                        # integer between 2 and 64\n",
    "        SCI_EPOCHS = int(SCI_EPOCHS)                            # integer between (100, 500)\n",
    "        SCI_BN_MOMENTUM = round(SCI_BN_MOMENTUM,2)              # real with two decimals between (0, 0.99)\n",
    "        SCI_SGD_MOMENTUM = round(SCI_SGD_MOMENTUM,2)            # real with two decimals between (0, 0.99) \n",
    "        SCI_optimizer = int(SCI_optimizer)                      # integer between 1 and 4\n",
    "        SCI_loss_type = int(SCI_loss_type)                      # integer between 1 and 3 ('CrossEntropyLoss', 'MultiMarginLoss','NLLLoss')\n",
    "        if int(SCI_RELU) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_RELU = True      \n",
    "        else:\n",
    "            SCI_RELU = False      \n",
    "        if int(SCI_BIAS) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_BIAS = True      \n",
    "        else:\n",
    "            SCI_BIAS = False  \n",
    "               \n",
    "        from cnn_model import CNN6\n",
    "        cnn = CNN6(L_FIRST, SCI_L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, SCI_DROPOUT, dataset.CLASSES)     \n",
    "    \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim = 0) \n",
    "            cnn = cnn.cuda()                \n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        #next(cnn.parameters()).is_cuda\n",
    "        #print(cnn)  # net architecture   \n",
    "        #list(cnn.parameters()) \n",
    "        cnn.apply(CNN6.weights_reset)        \n",
    "        cnn.share_memory()\n",
    "     \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 1:\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "            if LOSS == 2:\n",
    "                loss_func = nn.NLLLoss()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        #optimizer = str(SCI_optimizer)\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "    \n",
    "        from adamw import AdamW\n",
    "        \n",
    "        \n",
    "        if SCI_optimizer == 1:\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 2:\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION, amsgrad=True)\n",
    "        if SCI_optimizer == 3:\n",
    "            optimizer = AdamW(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay = REGULARIZATION)           \n",
    "        if SCI_optimizer == 4:\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=LR, momentum=SCI_SGD_MOMENTUM, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 5:\n",
    "            optimizer = optim.Adadelta(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 6:\n",
    "            optimizer = optim.Adagrad(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "    \n",
    "        from Utillities import Utillities\n",
    "        Utillities.listing(optimizer, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, SCI_L_SECOND, SCI_LR, SCI_RELU, SCI_BIAS, SCI_loss_type, REGULARIZATION, SCI_BATCH_SIZE, SCI_DROPOUT)\n",
    "\n",
    "    \n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = SCI_BATCH_SIZE, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = SCI_BATCH_SIZE, shuffle = True, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    ps = torch.exp(output)\n",
    "                    equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "        \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    #cnn = TheModelClass(*args, **kwargs)\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt'))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])   \n",
    "            print('Class: ',i,' correct: ', class_correct[i],' of ',dataset.TESTED_ELEMENTS[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1] * 5)\n",
    "        \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 300\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "        print('Credit Cost: ',-CreditCost)\n",
    "        #list(cnn.parameters())\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        if -CreditCost > MaxCredit : \n",
    "            MaxCredit = -CreditCost\n",
    "        print('Best Score So Far: ',MaxCredit)    \n",
    "        \n",
    "        return -CreditCost\n",
    "    \n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        #pbounds=pbounds,\n",
    "        pbounds={'SCI_RELU': (1,2.99), 'SCI_BIAS': (1,2.99), 'SCI_loss_type': (1, 3.99), 'SCI_optimizer': (1, 6.99),'SCI_LR': (0.001, 0.5), 'SCI_MM': (0.001, 0.999), 'SCI_REGULARIZATION': (0.0001, 0.7), 'SCI_EPOCHS': (1000, 2000), 'SCI_BATCH_SIZE': (4, 256), 'SCI_DROPOUT': (0, 0.5), 'SCI_L_SECOND': (1, 96), 'SCI_BN_MOMENTUM': (0, 0.99), 'SCI_SGD_MOMENTUM': (0, 0.99)},\n",
    "        verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "        random_state=1,\n",
    "    )\n",
    "        \n",
    "\n",
    "    #optimizer.maximize(\n",
    "        #n_iter=TRIALS, acq=\"ucb\", kappa=0.1\n",
    "    #)\n",
    "    \n",
    "    \n",
    "    optimizer.maximize(\n",
    "        init_points = RANDOM_STARTS,\n",
    "        n_iter = TRIALS,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(optimizer.max)\n",
    "    \n",
    "    for i, res in enumerate(optimizer.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCI_SGD_MOMENTUM:  tensor([0.7576], requires_grad=True)\n",
      "SCI_BATCH_SIZE:  tensor([[61.]], requires_grad=True)\n",
      "SCI_L_SECOND:  tensor([[10.]], requires_grad=True)\n",
      "SCI_optimizer:  tensor([[4.]], requires_grad=True)\n",
      "SCI_DROPOUT:  tensor([0.0293], requires_grad=True)\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  10\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  61\n",
      "Dropout:  0.014640778303146362\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  11\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0751]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0751]], grad_fn=<NegBackward>)\n",
      "t = 0, loss = 499.07513427734375, SCI_DROPOUT = [0.01928472], SCI_SGD_MOMENTUM = 0.747663140296936, SCI_BATCH_SIZE = [[60.500156]], SCI_L_SECOND = [[9.500158]], SCI_optimizer = [[3.7500792]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  9\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  60\n",
      "Dropout:  0.009642358869314194\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0739]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0739]], grad_fn=<NegBackward>)\n",
      "t = 1, loss = 499.0738525390625, SCI_DROPOUT = [0.00928696], SCI_SGD_MOMENTUM = 0.7376854419708252, SCI_BATCH_SIZE = [[60.000267]], SCI_L_SECOND = [[9.00027]], SCI_optimizer = [[3.5001352]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  9\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  60\n",
      "Dropout:  0.004643477965146303\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0726]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0726]], grad_fn=<NegBackward>)\n",
      "t = 2, loss = 499.07257080078125, SCI_DROPOUT = [-0.00071122], SCI_SGD_MOMENTUM = 0.7277036905288696, SCI_BATCH_SIZE = [[59.50036]], SCI_L_SECOND = [[8.500361]], SCI_optimizer = [[3.2501807]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  8\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  59\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0714]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0714]], grad_fn=<NegBackward>)\n",
      "t = 3, loss = 499.07135009765625, SCI_DROPOUT = [-0.01070964], SCI_SGD_MOMENTUM = 0.7177194952964783, SCI_BATCH_SIZE = [[59.00044]], SCI_L_SECOND = [[8.000441]], SCI_optimizer = [[3.0002203]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  8\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  59\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0701]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0701]], grad_fn=<NegBackward>)\n",
      "t = 4, loss = 499.070068359375, SCI_DROPOUT = [-0.02070822], SCI_SGD_MOMENTUM = 0.7077336311340332, SCI_BATCH_SIZE = [[58.50051]], SCI_L_SECOND = [[7.500511]], SCI_optimizer = [[2.7502556]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  7\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  58\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0688]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0688]], grad_fn=<NegBackward>)\n",
      "t = 5, loss = 499.06878662109375, SCI_DROPOUT = [-0.03070693], SCI_SGD_MOMENTUM = 0.6977465152740479, SCI_BATCH_SIZE = [[58.000576]], SCI_L_SECOND = [[7.000576]], SCI_optimizer = [[2.500288]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  7\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  58\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0676]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0676]], grad_fn=<NegBackward>)\n",
      "t = 6, loss = 499.06756591796875, SCI_DROPOUT = [-0.04070573], SCI_SGD_MOMENTUM = 0.6877584457397461, SCI_BATCH_SIZE = [[57.500637]], SCI_L_SECOND = [[6.500636]], SCI_optimizer = [[2.250318]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  6\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  57\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0663]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0663]], grad_fn=<NegBackward>)\n",
      "t = 7, loss = 499.0662841796875, SCI_DROPOUT = [-0.05070461], SCI_SGD_MOMENTUM = 0.677769660949707, SCI_BATCH_SIZE = [[57.000694]], SCI_L_SECOND = [[6.0006924]], SCI_optimizer = [[2.0003462]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  6\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  57\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0650]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0650]], grad_fn=<NegBackward>)\n",
      "t = 8, loss = 499.06500244140625, SCI_DROPOUT = [-0.06070355], SCI_SGD_MOMENTUM = 0.6677802205085754, SCI_BATCH_SIZE = [[56.500748]], SCI_L_SECOND = [[5.5007453]], SCI_optimizer = [[1.7503726]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  5\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  56\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0638]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0638]], grad_fn=<NegBackward>)\n",
      "t = 9, loss = 499.06378173828125, SCI_DROPOUT = [-0.07070255], SCI_SGD_MOMENTUM = 0.6577902436256409, SCI_BATCH_SIZE = [[56.000797]], SCI_L_SECOND = [[5.0007954]], SCI_optimizer = [[1.5003977]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  5\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  56\n",
      "Dropout:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  499.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[800.0625]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0638]], grad_fn=<NegBackward>)\n",
      "t = 10, loss = 500.0625, SCI_DROPOUT = [-0.08070159], SCI_SGD_MOMENTUM = 0.6477997899055481, SCI_BATCH_SIZE = [[55.500847]], SCI_L_SECOND = [[4.500843]], SCI_optimizer = [[1.2504215]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  55\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  10\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0612]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0612]], grad_fn=<NegBackward>)\n",
      "t = 11, loss = 499.06121826171875, SCI_DROPOUT = [-0.09070067], SCI_SGD_MOMENTUM = 0.6378089189529419, SCI_BATCH_SIZE = [[55.000893]], SCI_L_SECOND = [[4.000889]], SCI_optimizer = [[1.0004444]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  55\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0600]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0600]], grad_fn=<NegBackward>)\n",
      "t = 12, loss = 499.05999755859375, SCI_DROPOUT = [-0.10069979], SCI_SGD_MOMENTUM = 0.627817690372467, SCI_BATCH_SIZE = [[54.50094]], SCI_L_SECOND = [[3.500933]], SCI_optimizer = [[0.75046647]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  54\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  14\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0594]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0594]], grad_fn=<NegBackward>)\n",
      "t = 13, loss = 499.0594482421875, SCI_DROPOUT = [-0.11069894], SCI_SGD_MOMENTUM = 0.6178261637687683, SCI_BATCH_SIZE = [[54.00098]], SCI_L_SECOND = [[3.0492637]], SCI_optimizer = [[0.52463186]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  54\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  11\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0590]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0590]], grad_fn=<NegBackward>)\n",
      "t = 14, loss = 499.0589599609375, SCI_DROPOUT = [-0.12069812], SCI_SGD_MOMENTUM = 0.6078343391418457, SCI_BATCH_SIZE = [[53.501022]], SCI_L_SECOND = [[2.640513]], SCI_optimizer = [[0.32025647]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  53\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  499.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[800.0584]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0590]], grad_fn=<NegBackward>)\n",
      "t = 15, loss = 500.05841064453125, SCI_DROPOUT = [-0.13069732], SCI_SGD_MOMENTUM = 0.597842276096344, SCI_BATCH_SIZE = [[53.00106]], SCI_L_SECOND = [[2.2700815]], SCI_optimizer = [[0.13504079]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  53\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0579]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0579]], grad_fn=<NegBackward>)\n",
      "t = 16, loss = 499.05792236328125, SCI_DROPOUT = [-0.14069656], SCI_SGD_MOMENTUM = 0.5878499746322632, SCI_BATCH_SIZE = [[52.5011]], SCI_L_SECOND = [[1.933991]], SCI_optimizer = [[-0.03300448]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  52\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0574]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0574]], grad_fn=<NegBackward>)\n",
      "t = 17, loss = 499.05743408203125, SCI_DROPOUT = [-0.1506958], SCI_SGD_MOMENTUM = 0.5778574347496033, SCI_BATCH_SIZE = [[52.001137]], SCI_L_SECOND = [[1.6287711]], SCI_optimizer = [[-0.18561442]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  52\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  7\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0569]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0569]], grad_fn=<NegBackward>)\n",
      "t = 18, loss = 499.056884765625, SCI_DROPOUT = [-0.16069508], SCI_SGD_MOMENTUM = 0.567864716053009, SCI_BATCH_SIZE = [[51.501175]], SCI_L_SECOND = [[1.3513739]], SCI_optimizer = [[-0.32431304]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  51\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  10\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0564]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0564]], grad_fn=<NegBackward>)\n",
      "t = 19, loss = 499.056396484375, SCI_DROPOUT = [-0.17069437], SCI_SGD_MOMENTUM = 0.5578718185424805, SCI_BATCH_SIZE = [[51.00121]], SCI_L_SECOND = [[1.0991064]], SCI_optimizer = [[-0.45044678]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  51\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0559]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0559]], grad_fn=<NegBackward>)\n",
      "t = 20, loss = 499.055908203125, SCI_DROPOUT = [-0.18069367], SCI_SGD_MOMENTUM = 0.5478787422180176, SCI_BATCH_SIZE = [[50.501244]], SCI_L_SECOND = [[0.8695773]], SCI_optimizer = [[-0.56521136]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  50\n",
      "Dropout:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0554]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0554]], grad_fn=<NegBackward>)\n",
      "t = 21, loss = 499.05535888671875, SCI_DROPOUT = [-0.19069299], SCI_SGD_MOMENTUM = 0.5378854870796204, SCI_BATCH_SIZE = [[50.001278]], SCI_L_SECOND = [[0.6606536]], SCI_optimizer = [[-0.6696732]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  50\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0549]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0549]], grad_fn=<NegBackward>)\n",
      "t = 22, loss = 499.05487060546875, SCI_DROPOUT = [-0.20069233], SCI_SGD_MOMENTUM = 0.5278921127319336, SCI_BATCH_SIZE = [[49.501312]], SCI_L_SECOND = [[0.47042602]], SCI_optimizer = [[-0.764787]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  49\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0544]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0544]], grad_fn=<NegBackward>)\n",
      "t = 23, loss = 499.05438232421875, SCI_DROPOUT = [-0.21069168], SCI_SGD_MOMENTUM = 0.5178986191749573, SCI_BATCH_SIZE = [[49.001347]], SCI_L_SECOND = [[0.29717982]], SCI_optimizer = [[-0.8514101]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  49\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  8\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0538]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0538]], grad_fn=<NegBackward>)\n",
      "t = 24, loss = 499.0538330078125, SCI_DROPOUT = [-0.22069104], SCI_SGD_MOMENTUM = 0.5079050064086914, SCI_BATCH_SIZE = [[48.501377]], SCI_L_SECOND = [[0.13937096]], SCI_optimizer = [[-0.93031454]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  48\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  17\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0533]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0533]], grad_fn=<NegBackward>)\n",
      "t = 25, loss = 499.0533447265625, SCI_DROPOUT = [-0.23069042], SCI_SGD_MOMENTUM = 0.4979112446308136, SCI_BATCH_SIZE = [[48.001408]], SCI_L_SECOND = [[-0.00439417]], SCI_optimizer = [[-1.0021971]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  48\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  35\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0528]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0528]], grad_fn=<NegBackward>)\n",
      "t = 26, loss = 499.05279541015625, SCI_DROPOUT = [-0.2406898], SCI_SGD_MOMENTUM = 0.48791736364364624, SCI_BATCH_SIZE = [[47.50144]], SCI_L_SECOND = [[-0.13537595]], SCI_optimizer = [[-1.067688]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  47\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  12\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0523]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0523]], grad_fn=<NegBackward>)\n",
      "t = 27, loss = 499.05230712890625, SCI_DROPOUT = [-0.2506892], SCI_SGD_MOMENTUM = 0.4779233932495117, SCI_BATCH_SIZE = [[47.00147]], SCI_L_SECOND = [[-0.25471607]], SCI_optimizer = [[-1.1273581]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  47\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  100.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  10\n",
      "\n",
      "\n",
      "Credit Cost:  tensor([[799.0518]], grad_fn=<AddBackward0>)\n",
      "Best Score So Far:  tensor([[-799.0518]], grad_fn=<NegBackward>)\n",
      "t = 28, loss = 499.05181884765625, SCI_DROPOUT = [-0.2606886], SCI_SGD_MOMENTUM = 0.46792930364608765, SCI_BATCH_SIZE = [[46.5015]], SCI_L_SECOND = [[-0.36345032]], SCI_optimizer = [[-1.1817251]]\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.35\n",
      "    weight_decay: 0.03\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "LR:  0.35\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "REGULARIZATION:  0.03\n",
      "BATCH_SIZE:  46\n",
      "Dropout:  0\n",
      "Loaded the model with the lowest Validation Loss!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae41aa122034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0moptim_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCI_SGD_MOMENTUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCI_DROPOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCI_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCI_L_SECOND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCI_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ae41aa122034>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND, SCI_optimizer)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                     \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "if OPTIMIZATION_PLUGIN == 'GradDescent' :\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import torch.optim as optim\n",
    "    from torch.autograd import Variable\n",
    "  \n",
    "\n",
    "   \n",
    "    #CI_LR = torch.randn(1).detach().requires_grad_(True)\n",
    "    SCI_LR = 0.35\n",
    "    SCI_REGULARIZATION = 0.03\n",
    "    SCI_EPOCHS = 200\n",
    "    #SCI_optimizer = 'Adam'\n",
    "    SCI_loss_type = 'CrossEntropyLoss'\n",
    "    SCI_RELU = 'True'\n",
    "    SCI_BIAS = 'True'\n",
    "    #SCI_L_SECOND = 48\n",
    "    SCI_BN_MOMENTUM = 0.1\n",
    "\n",
    "    SCI_SGD_MOMENTUM = torch.rand(1, requires_grad=True)\n",
    "    print('SCI_SGD_MOMENTUM: ', SCI_SGD_MOMENTUM)\n",
    "    SCI_BATCH_SIZE   = torch.randint(2, 128, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_BATCH_SIZE: ',SCI_BATCH_SIZE)\n",
    "    SCI_L_SECOND   = torch.randint(2, 64, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_L_SECOND: ',SCI_L_SECOND)\n",
    "    SCI_optimizer   = torch.randint(1, 6, (1,1), dtype=torch.float, requires_grad=True) \n",
    "    print('SCI_optimizer: ',SCI_optimizer)    \n",
    "    SCI_DROPOUT      = torch.rand(1, requires_grad=True)    \n",
    "    print('SCI_DROPOUT: ',SCI_DROPOUT)   \n",
    "    \n",
    "\n",
    "    def objective(SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND, SCI_optimizer):\n",
    "        global SCI_REGULARIZATION, SCI_EPOCHS, SCI_loss_type, SCI_RELU\n",
    "        global SCI_BIAS, SCI_BN_MOMENTUM, device, SCI_LR, MaxCredit\n",
    "        \n",
    "        SCI_SGD_MOMENTUM = SCI_SGD_MOMENTUM/10\n",
    "        DROPOUT = (SCI_DROPOUT/2).item()\n",
    "        if SCI_DROPOUT < 0 :\n",
    "            DROPOUT = 0\n",
    "\n",
    "        BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        \n",
    "        if SCI_L_SECOND < 4 :\n",
    "            SCI_L_SECOND = 4\n",
    "            \n",
    "        if SCI_optimizer < 1 :\n",
    "            SCI_optimizer = 1\n",
    "        \n",
    "        L_SECOND = int(SCI_L_SECOND)\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 'CrossEntropyLoss':\n",
    "                loss_func = nn.CrossEntropyLoss()\n",
    "            if LOSS == 'NLLLoss':\n",
    "                loss_func = nn.NLLLoss()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss()\n",
    "            return loss_func\n",
    "\n",
    "\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        optimizer1 = str(SCI_optimizer)\n",
    "\n",
    "        from cnn_model import CNN6      \n",
    "        cnn = CNN6(L_FIRST, L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, DROPOUT, dataset.CLASSES)     \n",
    "    \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn,device_ids=[0, 1], dim=0) \n",
    "            cnn = cnn.cuda()\n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.apply(CNN6.weights_reset)\n",
    "        cnn.share_memory()\n",
    "\n",
    "\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "        \n",
    "        from adamw import AdamW\n",
    "        \n",
    "        \n",
    "        if optimizer1 == '1':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=SCI_LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "        if optimizer1 == '2':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=SCI_LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION, amsgrad=True)\n",
    "        if optimizer1 == '3':\n",
    "            optimizer = AdamW(cnn.parameters(), lr=SCI_LR, betas=(0.9, 0.99), weight_decay = REGULARIZATION)            \n",
    "        if optimizer1 == '4':\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=SCI_LR, momentum=SCI_SGD_MOMENTUM, weight_decay=REGULARIZATION)\n",
    "        if optimizer1 == '5':\n",
    "            optimizer = optim.Adadelta(cnn.parameters(), lr=SCI_LR, weight_decay=REGULARIZATION)\n",
    "        if optimizer1 == '6':\n",
    "            optimizer = optim.Adagrad(cnn.parameters(), lr=SCI_LR, weight_decay=REGULARIZATION)\n",
    "        if optimizer1  > '6':           \n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=SCI_LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "\n",
    "    \n",
    "        from Utillities import Utillities\n",
    "        Utillities.listing(optimizer, SCI_SGD_MOMENTUM, SCI_BN_MOMENTUM, L_SECOND, SCI_LR, SCI_RELU, SCI_BIAS, SCI_loss_type, REGULARIZATION, BATCH_SIZE, DROPOUT)\n",
    "\n",
    "        train_loader = Data.DataLoader(dataset = dataset.train_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)\n",
    "        validation_loader = Data.DataLoader(dataset = dataset.validation_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True, pin_memory=True)    \n",
    "        test_loader = Data.DataLoader(dataset = dataset.test_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0, pin_memory=True)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    ps = torch.exp(output)\n",
    "                    equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "       \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(dataset.CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / dataset.TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])\n",
    "            print('Class: ',i,' correct: ', class_correct[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = (1 - TEST_RESULTS[0,0]) * dataset.TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * dataset.TESTED_ELEMENTS[1] * 5\n",
    "    \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 300\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "   \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        \n",
    "        CreditCost = CreditCost + (SCI_SGD_MOMENTUM + SCI_DROPOUT + SCI_BATCH_SIZE + SCI_L_SECOND + SCI_optimizer)/1000\n",
    "        print('Credit Cost: ',CreditCost)\n",
    "        \n",
    "        \n",
    "        if -CreditCost > MaxCredit : \n",
    "            MaxCredit = -CreditCost\n",
    "        print('Best Score So Far: ',MaxCredit)   \n",
    "             \n",
    "        return CreditCost\n",
    "\n",
    "    \n",
    "    def loss(y_predicted, expected):\n",
    "        return (y_predicted - expected).sum()\n",
    "            \n",
    "    \n",
    "    expected = 300\n",
    "    \n",
    "    #optim_alg = optim.Adagrad([SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND], lr=0.01)\n",
    "    optim_alg = optim.Adam([       \n",
    "        {'params': SCI_SGD_MOMENTUM, 'lr': 1e-2},\n",
    "        {'params': SCI_DROPOUT, 'lr': 1e-2},\n",
    "        {'params': SCI_BATCH_SIZE, 'lr': 0.5},\n",
    "        {'params': SCI_L_SECOND, 'lr': 0.5},\n",
    "        {'params': SCI_optimizer, 'lr': 0.25}\n",
    "        ]) \n",
    "    \n",
    "    # Main optimization loop\n",
    "    for t in range(50):\n",
    "        optim_alg.zero_grad()\n",
    "        y_predicted = objective(SCI_SGD_MOMENTUM, SCI_DROPOUT, SCI_BATCH_SIZE, SCI_L_SECOND, SCI_optimizer)\n",
    "        current_loss = loss(y_predicted, expected)\n",
    "        current_loss.backward()\n",
    "        optim_alg.step()\n",
    "        print(f\"t = {t}, loss = {current_loss}, SCI_DROPOUT = {SCI_DROPOUT.detach().numpy()}, SCI_SGD_MOMENTUM = {SCI_SGD_MOMENTUM.item()}, SCI_BATCH_SIZE = {SCI_BATCH_SIZE.detach().numpy()}, SCI_L_SECOND = {SCI_L_SECOND.detach().numpy()}, SCI_optimizer = {SCI_optimizer.detach().numpy()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end.record()\n",
    "\n",
    "#print('Minimum Credit Cost: ',Min_Credit_Cost)\n",
    "\n",
    "print()\n",
    "print('Total execution time (minutes): ',start.elapsed_time(end)/60000)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = SortKey.CUMULATIVE\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
