{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.1.0\n",
      "Torchvision Version:  0.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "#print(torch.cuda.nccl.is_available())\n",
    "torch.manual_seed(0)   # reproducible\n",
    "\n",
    "\n",
    "OPTIMIZATION_PLUGIN = 'Scikit' # 'Bayesian' or 'Scikit'\n",
    "GET_STATS = False\n",
    "GPU_SELECT = 2 # can be 0, 1, 2 (both)\n",
    "PARALLEL_PROCESSES = 2\n",
    "TRIALS = 20\n",
    "RANDOM_STARTS = 10\n",
    "LR  = 1e-5                # learning rate\n",
    "SCI_LR =  1e-5\n",
    "LR2 = 1e-5\n",
    "SCI_MM = 0.5                 # momentum - used only with SGD optimizer\n",
    "MM = 0.5\n",
    "L_FIRST = 1\n",
    "KERNEL_X = 7\n",
    "patience = 12             # if validation loss not going down, wait \"patience\" number of epochs\n",
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 NVIDIA 1080TI GPUs!\n"
     ]
    }
   ],
   "source": [
    "#DATASET\n",
    "\n",
    "CLASSES = 2\n",
    "TRAIN_SIZE = 256\n",
    "VALIDATION_SIZE = 90\n",
    "TEST_SIZE = 654\n",
    "TESTED_ELEMENTS = torch.tensor([527,127]).type(torch.FloatTensor) \n",
    "TEST_RESULTS = torch.zeros(1,2)\n",
    "LAST_DATA_ELEMENT = 24\n",
    "file = ['train6.csv','validate6.csv','test6.csv']\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.enable()\n",
    "    \n",
    "\n",
    "if GPU_SELECT == 2:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using\", torch.cuda.device_count(), \"NVIDIA 1080TI GPUs!\")\n",
    "\n",
    "if GPU_SELECT == 1:\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")    \n",
    "    print(\"Using one (the second) NVIDIA 1080TI GPU!\")\n",
    "\n",
    "if GPU_SELECT == 0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")       \n",
    "    print(\"Using one (the first) NVIDIA 1080TI GPU!\")\n",
    "\n",
    "\n",
    "def CreateDataset(file, SIZE, LAST_DATA_COLUMN):\n",
    "    file_reader = pd.read_csv(file, header=None)\n",
    "    temp_tensor = torch.tensor(file_reader.values)\n",
    "\n",
    "    target = torch.zeros(SIZE, device = device)\n",
    "    target = temp_tensor[:,LAST_DATA_COLUMN]\n",
    "    target.requires_grad = False\n",
    "    target = torch.t(target).type(torch.LongTensor).cuda()\n",
    "\n",
    "    data = torch.zeros(1,1,SIZE,LAST_DATA_COLUMN, device = device)\n",
    "    data[0,0,:,:] = temp_tensor[:,0:LAST_DATA_COLUMN]\n",
    "    data = data.permute(2,1,3,0)\n",
    "    data.requires_grad = False\n",
    "\n",
    "    return data, target\n",
    "\n",
    "train_data, train_target = CreateDataset(file[0], TRAIN_SIZE, LAST_DATA_ELEMENT)\n",
    "validation_data, validation_target = CreateDataset(file[1], VALIDATION_SIZE, LAST_DATA_ELEMENT)\n",
    "test_data, test_target = CreateDataset(file[2], TEST_SIZE, LAST_DATA_ELEMENT)\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_data, train_target)\n",
    "validation_dataset = Data.TensorDataset(validation_data, validation_target)\n",
    "test_dataset = Data.TensorDataset(test_data, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)        \n",
    "\n",
    "            \n",
    "def weights_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)  # initialize the early_stopping object\n",
    "\n",
    "# Counter for the execution time\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Optimization:  Adagrad (\n",
      "Parameter Group 0\n",
      "    initial_accumulator_value: 0\n",
      "    lr: 0.09\n",
      "    lr_decay: 0\n",
      "    weight_decay: 0.3\n",
      ")\n",
      "Batch Normalization Momentum:  0\n",
      "Nodes:  20\n",
      "RELU:  False\n",
      "BIAS:  False\n",
      "Loss Type:  NLLLoss\n",
      "BATCH_SIZE:  128\n",
      "Dropout:  0.2\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  19999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 214.0078\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 835.0000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Optimization:  Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-06\n",
      "    lr: 0.08\n",
      "    rho: 0.9\n",
      "    weight_decay: 0.0007\n",
      ")\n",
      "Batch Normalization Momentum:  0.5\n",
      "Nodes:  24\n",
      "RELU:  False\n",
      "BIAS:  True\n",
      "Loss Type:  MultiMarginLoss\n",
      "BATCH_SIZE:  192\n",
      "Dropout:  0.37\n",
      "Class:  0  accuracy:  tensor(0.)\n",
      "Class:  0  correct:  0.0\n",
      "Class:  1  accuracy:  tensor(1.)\n",
      "Class:  1  correct:  127.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  19999\n",
      "Credit Cost:  727\n",
      "\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 139.9403\n",
      "Function value obtained: 727.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.3\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  24\n",
      "RELU:  True\n",
      "BIAS:  False\n",
      "Loss Type:  NLLLoss\n",
      "BATCH_SIZE:  160\n",
      "Dropout:  0.1\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 62.8910\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.07\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Batch Normalization Momentum:  0.01\n",
      "Nodes:  16\n",
      "RELU:  True\n",
      "BIAS:  False\n",
      "Loss Type:  CrossEntropyLoss\n",
      "BATCH_SIZE:  48\n",
      "Dropout:  0.3\n",
      "Loaded the model with the lowest Validation Loss!\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  14\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.4540\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.3\n",
      "    weight_decay: 0.0003\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  8\n",
      "RELU:  False\n",
      "BIAS:  True\n",
      "Loss Type:  NLLLoss\n",
      "BATCH_SIZE:  128\n",
      "Dropout:  0.33\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  19999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 200.3576\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Optimization:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.08\n",
      "    momentum: 0.8\n",
      "    nesterov: False\n",
      "    weight_decay: 0.3\n",
      ")\n",
      "Batch Normalization Momentum:  0.1\n",
      "Nodes:  4\n",
      "RELU:  True\n",
      "BIAS:  True\n",
      "Loss Type:  MultiMarginLoss\n",
      "BATCH_SIZE:  160\n",
      "Dropout:  0.2\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  9999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 49.4343\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Optimization:  AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.3\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "Batch Normalization Momentum:  0.9\n",
      "Nodes:  32\n",
      "RELU:  False\n",
      "BIAS:  True\n",
      "Loss Type:  CrossEntropyLoss\n",
      "BATCH_SIZE:  256\n",
      "Dropout:  0\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  19999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 121.5563\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.7\n",
      "    weight_decay: 0.7\n",
      ")\n",
      "Batch Normalization Momentum:  0.9\n",
      "Nodes:  64\n",
      "RELU:  True\n",
      "BIAS:  False\n",
      "Loss Type:  CrossEntropyLoss\n",
      "BATCH_SIZE:  96\n",
      "Dropout:  0.23\n",
      "Class:  0  accuracy:  tensor(1.)\n",
      "Class:  0  correct:  527.0\n",
      "Class:  1  accuracy:  tensor(0.)\n",
      "Class:  1  correct:  0.0\n",
      "Final percentage:  tensor(0.5000)\n",
      "Last epoch:  19999\n",
      "Credit Cost:  835\n",
      "\n",
      "\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 210.9724\n",
      "Function value obtained: 835.0000\n",
      "Current minimum: 727.0000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Optimization:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0.0007\n",
      ")\n",
      "Batch Normalization Momentum:  0.6\n",
      "Nodes:  20\n",
      "RELU:  False\n",
      "BIAS:  True\n",
      "Loss Type:  NLLLoss\n",
      "BATCH_SIZE:  224\n",
      "Dropout:  0\n"
     ]
    }
   ],
   "source": [
    "if OPTIMIZATION_PLUGIN == 'Scikit' :\n",
    "    from skopt import gp_minimize\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from skopt.space import Real, Integer\n",
    "    from skopt.utils import use_named_args\n",
    "    from skopt.plots import plot_convergence\n",
    "    from functools import partial\n",
    "    from skopt.plots import plot_evaluations\n",
    "    from skopt import gp_minimize, forest_minimize, dummy_minimize, gbrt_minimize\n",
    "    from skopt.plots import plot_objective\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import CategoricalEncoder\n",
    "    from skopt.space import Real, Categorical, Integer\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    "\n",
    "    #SCI_LR = Categorical(categories=[1e-1, 3e-1, 5e-1, 7e-1, 1e-2, 3e-2, 5e-2, 7e-2, 1e-3, 3e-3, 5e-3, 7e-3, 1e-4, 3e-4, 0.1, 0.2, 0.3, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.001, 0.0001, 1e-5],name= 'SCI_LR')\n",
    "    SCI_LR = Categorical(categories=[1e-1, 3e-1, 5e-1, 7e-1, 1e-2, 3e-2, 5e-2, 7e-2, 0.1, 0.2, 0.3, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.001],name= 'SCI_LR')\n",
    "    SCI_MM = Categorical(categories=[0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999], name='SCI_MM')\n",
    "    SCI_REGULARIZATION = Categorical(categories=[0.0001, 0.0003, 0.0007, 0.001, 0.003, 0.007, 0.01, 0.03, 0.07, 0.1, 0.3, 0.7], name='SCI_REGULARIZATION')\n",
    "    SCI_EPOCHS = Categorical(categories=[20000, 10000], name='SCI_EPOCHS')\n",
    "    SCI_optimizer = Categorical(categories=['Adam', 'Adadelta', 'SGD', 'Adagrad', 'AMSGrad', 'AdamW'],name='SCI_optimizer') #\n",
    "    SCI_loss_type = Categorical(categories=['CrossEntropyLoss', 'MultiMarginLoss','NLLLoss'],name='SCI_loss_type') # \n",
    "    SCI_BATCH_SIZE = Categorical(categories=[4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 160, 192, 224, 256], name='SCI_BATCH_SIZE')\n",
    "    SCI_DROPOUT = Categorical(categories=[0, 0.01, 0.03, 0.07, 0.1, 0.13, 0.17, 0.2, 0.23, 0.27, 0.3, 0.33, 0.37, 0.4] , name = 'SCI_DROPOUT')\n",
    "    SCI_RELU = Categorical(categories=['True', 'False'] , name = 'SCI_RELU')\n",
    "    SCI_BIAS = Categorical(categories=['True', 'False'] , name = 'SCI_BIAS')\n",
    "    SCI_L_SECOND = Categorical(categories=[2, 4, 6, 8, 12, 16, 20, 24, 32, 48, 64], name='SCI_L_SECOND')\n",
    "    SCI_BN_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_BN_MOMENTUM') \n",
    "    SCI_SGD_MOMENTUM = Categorical(categories=[0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99] , name = 'SCI_SGD_MOMENTUM') \n",
    "\n",
    "    dimensions = [SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM]\n",
    "\n",
    "    @use_named_args(dimensions = dimensions)\n",
    "\n",
    "    def objective(SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_optimizer, SCI_LR, SCI_loss_type, SCI_DROPOUT, SCI_RELU, SCI_BIAS, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM):\n",
    "        global device  \n",
    "\n",
    "        from cnn_model import CNN4\n",
    "                \n",
    "        cnn = CNN4(L_FIRST, SCI_L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, SCI_DROPOUT, CLASSES)     \n",
    "    \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn) \n",
    "            cnn = cnn.cuda()\n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        cnn.share_memory()\n",
    "     \n",
    "        loss_func = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 'CrossEntropyLoss':\n",
    "                loss_func = nn.CrossEntropyLoss().cuda()\n",
    "            if LOSS == 'NLLLoss':\n",
    "                loss_func = nn.NLLLoss().cuda()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss().cuda()\n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        optimizer = str(SCI_optimizer)\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "\n",
    "        cnn.apply(weights_reset)\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "        \n",
    "        from adamw import AdamW\n",
    "        \n",
    "        \n",
    "        if SCI_optimizer == 'Adam':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'AMSGrad':\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION, amsgrad=True)\n",
    "        if SCI_optimizer == 'AdamW':\n",
    "            optimizer = AdamW(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay = REGULARIZATION)            \n",
    "        if SCI_optimizer == 'SGD':\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=LR, momentum=SCI_SGD_MOMENTUM, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'Adadelta':\n",
    "            optimizer = optim.Adadelta(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 'Adagrad':\n",
    "            optimizer = optim.Adagrad(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "    \n",
    "        print('Optimization: ', optimizer)\n",
    "        if optimizer == 'SGD':\n",
    "            print('MM: ',SCI_SGD_MOMENTUM)\n",
    "        print('Batch Normalization Momentum: ',SCI_BN_MOMENTUM)   \n",
    "        print('Nodes: ', SCI_L_SECOND)         \n",
    "        #print('LR: ', SCI_LR)         \n",
    "        print('RELU: ', SCI_RELU)       \n",
    "        print('BIAS: ', SCI_BIAS)   \n",
    "        print('Loss Type: ', SCI_loss_type)   \n",
    "        #print('REGULARIZATION: ', REGULARIZATION)    \n",
    "        print('BATCH_SIZE: ', SCI_BATCH_SIZE)\n",
    "        print('Dropout: ', SCI_DROPOUT)\n",
    "    \n",
    "        #SCI_BATCH_SIZE = 1\n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True)\n",
    "        validation_loader = Data.DataLoader(dataset = validation_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True)    \n",
    "        test_loader = Data.DataLoader(dataset = test_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    ps = torch.exp(output)\n",
    "                    equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "       \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])\n",
    "            print('Class: ',i,' correct: ', class_correct[i])\n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * TESTED_ELEMENTS[1] * 5)\n",
    "    \n",
    "        if TEST_RESULTS[0,0] == 0 or TEST_RESULTS[0,1] == 0 :\n",
    "            CreditCost = CreditCost + 200\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "        print('Credit Cost: ',CreditCost)\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        print()\n",
    "        \n",
    "        return CreditCost\n",
    "    \n",
    "    #   not working    #res_gp = gp_minimize(objective, dimensions=dimensions, n_calls=TRIALS, random_state=1, verbose=True, acq_func='gp_hedge', acq_optimizer='auto', n_jobs=1)\n",
    "    #res_gp = forest_minimize(objective, dimensions=dimensions, base_estimator='RF', n_calls=TRIALS, n_random_starts=RANDOM_STARTS, acq_func='EI', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=10000, xi=0.01, kappa=1.96, n_jobs=128)\n",
    "    res_gp = gbrt_minimize(objective, dimensions=dimensions, base_estimator='ET', n_calls=TRIALS, n_random_starts=RANDOM_STARTS, acq_func='LCB', x0=None, y0=None, random_state=None, verbose=True, callback=None, n_points=100, xi=0.01, kappa=1.96, n_jobs=8)\n",
    "    #res_gp = dummy_minimize(objective, dimensions=dimensions, n_calls=TRIALS, x0=None, y0=None, random_state=None, verbose=True, callback=None)      \n",
    "\n",
    "    \"Best score=%.4f\" % res_gp.fun\n",
    "    print(\"\"\"Best parameters: - optimization=%d\"\"\" % (res_gp.x[0]))\n",
    "  \n",
    "    print(res_gp)\n",
    "    plot_convergence(res_gp)\n",
    "    #plot_evaluations(res_gp)\n",
    "    #plot_objective(res_gp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxCredit = -800\n",
    "\n",
    "if OPTIMIZATION_PLUGIN == 'Bayesian' :\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    \n",
    "    #def black_box_function(x, y):\n",
    "    def objective(SCI_RELU, SCI_BIAS, SCI_loss_type, SCI_optimizer, SCI_BATCH_SIZE, SCI_MM, SCI_REGULARIZATION, SCI_LR, SCI_DROPOUT, SCI_L_SECOND, SCI_EPOCHS, SCI_BN_MOMENTUM, SCI_SGD_MOMENTUM):\n",
    "        global device, MaxCredit  \n",
    "        \n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)                    # integer between 4 and 256\n",
    "        SCI_MM = round(SCI_MM,3)                                # real with three decimals between (0.001, 0.999)\n",
    "        SCI_REGULARIZATION = round(SCI_REGULARIZATION,3)        # real with three decimals between (0.001, 0.7)\n",
    "        SCI_LR = round(SCI_LR,5)                                # real with five decimals between(1e-4, 7e-1)            \n",
    "        SCI_DROPOUT = round(SCI_DROPOUT,2)                      # real with two decimals between (0, 0.4)\n",
    "        SCI_L_SECOND = int(SCI_L_SECOND)                        # integer between 2 and 64\n",
    "        SCI_EPOCHS = int(SCI_EPOCHS)                            # integer between (100, 500)\n",
    "        SCI_BN_MOMENTUM = round(SCI_BN_MOMENTUM,2)              # real with two decimals between (0, 0.99)\n",
    "        SCI_SGD_MOMENTUM = round(SCI_SGD_MOMENTUM,2)            # real with two decimals between (0, 0.99) \n",
    "        SCI_optimizer = int(SCI_optimizer)                      # integer between 1 and 4\n",
    "        SCI_loss_type = int(SCI_loss_type)                      # integer between 1 and 3 ('CrossEntropyLoss', 'MultiMarginLoss','NLLLoss')\n",
    "        if int(SCI_RELU) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_RELU = True      \n",
    "        else:\n",
    "            SCI_RELU = False      \n",
    "        if int(SCI_BIAS) == 1 :                                 # integer between 1 and 2 ('True', 'False')\n",
    "            SCI_BIAS = True      \n",
    "        else:\n",
    "            SCI_BIAS = False  \n",
    "               \n",
    "        from cnn_model import CNN2\n",
    "        cnn = CNN4(L_FIRST, SCI_L_SECOND, KERNEL_X, SCI_BIAS, SCI_BN_MOMENTUM, SCI_RELU, SCI_DROPOUT, CLASSES)     \n",
    "    \n",
    "        if GPU_SELECT == 2:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                cnn = nn.DataParallel(cnn) \n",
    "        if GPU_SELECT == 1:\n",
    "            cnn.to(device)  \n",
    "        if GPU_SELECT == 0:\n",
    "            cnn.to(device)        \n",
    "\n",
    "        #next(cnn.parameters()).is_cuda\n",
    "        #print(cnn)  # net architecture   \n",
    "        #list(cnn.parameters()) \n",
    "        cnn.share_memory()\n",
    "     \n",
    "        loss_func = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        def create_loss(LOSS):   \n",
    "            if LOSS == 1:\n",
    "                loss_func = nn.CrossEntropyLoss().cuda()\n",
    "            if LOSS == 2:\n",
    "                loss_func = nn.NLLLoss().cuda()\n",
    "            else:\n",
    "                loss_func = nn.MultiMarginLoss().cuda()\n",
    "            return loss_func\n",
    "\n",
    "        MM = float(str(SCI_MM))\n",
    "        REGULARIZATION = float(str(SCI_REGULARIZATION))\n",
    "        #optimizer = str(SCI_optimizer)\n",
    "        LR = float(str(SCI_LR))\n",
    "        train_losses = []         # to track the training loss as the model trains\n",
    "        output = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        early_stopping.counter = 0\n",
    "        early_stopping.best_score = None\n",
    "        early_stopping.early_stop = False\n",
    "        early_stopping.verbose = False  \n",
    "        TEST_RESULTS = torch.zeros(1,2)\n",
    "\n",
    "        cnn.apply(weights_reset)\n",
    "    \n",
    "        loss_type = create_loss(SCI_loss_type)\n",
    "    \n",
    "        from adamw import AdamW\n",
    "        \n",
    "        \n",
    "        if SCI_optimizer == 1:\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 2:\n",
    "            optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay=REGULARIZATION, amsgrad=True)\n",
    "        if SCI_optimizer == 3:\n",
    "            optimizer = AdamW(cnn.parameters(), lr=LR, betas=(0.9, 0.99), weight_decay = REGULARIZATION)           \n",
    "        if SCI_optimizer == 4:\n",
    "            optimizer = optim.SGD(cnn.parameters(), lr=LR, momentum=SCI_SGD_MOMENTUM, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 5:\n",
    "            optimizer = optim.Adadelta(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "        if SCI_optimizer == 6:\n",
    "            optimizer = optim.Adagrad(cnn.parameters(), lr=LR, weight_decay=REGULARIZATION)\n",
    "    \n",
    "    \n",
    "        print('Optimization: ', optimizer)\n",
    "        if optimizer == 'SGD':\n",
    "            print('MM: ',SCI_SGD_MOMENTUM)\n",
    "        print('Batch Normalization Momentum: ',SCI_BN_MOMENTUM)   \n",
    "        print('Nodes: ', SCI_L_SECOND)         \n",
    "        print('LR: ', SCI_LR)         \n",
    "        print('RELU: ', SCI_RELU)       \n",
    "        print('BIAS: ', SCI_BIAS)   \n",
    "        print('Loss Type: ', SCI_loss_type)   \n",
    "        print('REGULARIZATION: ', REGULARIZATION)    \n",
    "        print('BATCH_SIZE: ', SCI_BATCH_SIZE)\n",
    "        print('Dropout: ', SCI_DROPOUT)\n",
    "    \n",
    "        # Data Loader for easy mini-batch return in training\n",
    "        SCI_BATCH_SIZE = int(SCI_BATCH_SIZE)\n",
    "        train_loader = Data.DataLoader(dataset = train_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True)\n",
    "        validation_loader = Data.DataLoader(dataset = validation_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0, drop_last=True)    \n",
    "        test_loader = Data.DataLoader(dataset = test_dataset, batch_size = SCI_BATCH_SIZE, shuffle = False, num_workers = 0)\n",
    "    \n",
    "        for epoch in range(SCI_EPOCHS):\n",
    "            loss = None        \n",
    "            cnn.train().cuda()\n",
    "            for step, (train_data, train_target) in enumerate(train_loader):   \n",
    "                train_data, train_target = train_data.to(device), train_target.to(device)\n",
    "                output, temp = cnn(train_data)                # forward pass: compute predicted outputs by passing inputs to the model     \n",
    "                loss = loss_func(output, train_target)\n",
    "                train_losses.append(loss.item())              # record training loss \n",
    "                loss.backward()                               # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()                              # perform a single optimization step (parameter update)\n",
    "      \n",
    "            cnn.eval().cuda()                 # switch to evaluation (no change) mode           \n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for step, (validation_data, validation_target) in enumerate(validation_loader):\n",
    "                    validation_data, validation_target = validation_data.to(device), validation_target.to(device)\n",
    "                    output, temp = cnn(validation_data)            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                    valid_loss += loss_func(output, validation_target).item()\n",
    "                    ps = torch.exp(output)\n",
    "                    equality = (validation_target[0].data == ps.max(dim=1)[1])\n",
    "                    accuracy += equality.type(torch.FloatTensor).mean()      \n",
    "               \n",
    "            train_losses = []\n",
    "            early_stopping(valid_loss, cnn)\n",
    "        \n",
    "            if early_stopping.early_stop:\n",
    "                if os.path.exists('checkpoint.pt'):\n",
    "                    #cnn = TheModelClass(*args, **kwargs)\n",
    "                    print(\"Loaded the model with the lowest Validation Loss!\")\n",
    "                    cnn.load_state_dict(torch.load('checkpoint.pt', map_location=\"cuda:1\"))  # Choose whatever GPU device number you want\n",
    "                    cnn.to(device)\n",
    "                break\n",
    "      \n",
    "        cnn.eval()\n",
    "        class_correct = list(0. for i in range(1000))\n",
    "        class_total = list(0. for i in range(1000))\n",
    "        with torch.no_grad():\n",
    "            for (test_data, test_target) in test_loader:\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "                outputs, temp = cnn(test_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                c = (predicted == test_target).squeeze()\n",
    "                for i in range(test_target.size(0)):\n",
    "                    label = test_target[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        for i in range(CLASSES):\n",
    "            TEST_RESULTS[0,i] = class_correct[i] / TESTED_ELEMENTS[i]\n",
    "            print('Class: ',i,' accuracy: ', TEST_RESULTS[0,i])   \n",
    "        percent = (TEST_RESULTS[0,0]+TEST_RESULTS[0,1])/2\n",
    "        print('Final percentage: ',percent)\n",
    "    \n",
    "        CreditCost = int((1 - TEST_RESULTS[0,0]) * TESTED_ELEMENTS[0] + (1 - TEST_RESULTS[0,1]) * TESTED_ELEMENTS[1] * 5)\n",
    "    \n",
    "        print('Last epoch: ', epoch)\n",
    "        print('Credit Cost: ',-CreditCost)\n",
    "        #list(cnn.parameters())\n",
    "    \n",
    "        if os.path.exists('checkpoint.pt'):  \n",
    "            os.remove('checkpoint.pt') \n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        if -CreditCost > MaxCredit : \n",
    "            MaxCredit = -CreditCost\n",
    "        print('Best Score So Far: ',MaxCredit)    \n",
    "        \n",
    "        return -CreditCost\n",
    "    \n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        #pbounds=pbounds,\n",
    "        pbounds={'SCI_RELU': (1,2.99), 'SCI_BIAS': (1,2.99), 'SCI_loss_type': (1, 3.99), 'SCI_optimizer': (1, 6.99),'SCI_LR': (0.00001, 0.01), 'SCI_MM': (0.001, 0.999), 'SCI_REGULARIZATION': (0.0001, 0.7), 'SCI_EPOCHS': (10000, 20000), 'SCI_BATCH_SIZE': (4, 256), 'SCI_DROPOUT': (0, 0.4), 'SCI_L_SECOND': (2, 64), 'SCI_BN_MOMENTUM': (0, 0.99), 'SCI_SGD_MOMENTUM': (0, 0.99)},\n",
    "        verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "        random_state=1,\n",
    "    )\n",
    "        \n",
    "\n",
    "    optimizer.maximize(\n",
    "        n_iter=1000, acq=\"ucb\", kappa=0.1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(optimizer.max)\n",
    "    \n",
    "    for i, res in enumerate(optimizer.res):\n",
    "        print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end.record()\n",
    "\n",
    "#print('Minimum Credit Cost: ',Min_Credit_Cost)\n",
    "\n",
    "print()\n",
    "print('Total execution time (minutes): ',start.elapsed_time(end)/60000)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if GET_STATS:\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = SortKey.CUMULATIVE\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
